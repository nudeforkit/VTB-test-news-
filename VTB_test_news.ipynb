{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n", 
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "!pip install hnswlib pip install sentence-transformers hnswlib\n",
        "import hnswlib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "\n",
        "# 1) –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "def _strip_markdown_links(s: str) -> str:\n",
        "    # [text](url) -> text\n",
        "    return re.sub(r\"\\[([^\\]]+)\\]\\((https?://[^)]+)\\)\", r\"\\1\", s)\n",
        "\n",
        "def _normalize_ws(s: str) -> str:\n",
        "    s = s.replace(\"\\u00a0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _is_time_line(line: str) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{1,2}:\\d{2}\", line))\n",
        "\n",
        "def _is_small_digit_line(line: str) -> bool:\n",
        "    return line.isdigit() and len(line) <= 2\n",
        "\n",
        "def _should_drop_line(line: str) -> bool:\n",
        "    if not line:\n",
        "        return True\n",
        "\n",
        "    line2 = _PUA_RE.sub(\"\", line).strip()\n",
        "    if not line2:\n",
        "        return True\n",
        "\n",
        "    # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏\n",
        "    if set(line2) <= set(\"-‚Äì‚Äî_\"):\n",
        "        return True\n",
        "\n",
        "    # –º–µ—Ç–æ–¥–∞–Ω–Ω—ã–µ\n",
        "    if line2.startswith(\"üì¢ From:\") or line2.startswith(\"üìÅ Category:\") or line2.startswith(\"üîó Link:\") or line2.startswith(\"‚è∞ Time:\"):\n",
        "        return True\n",
        "\n",
        "    # –º–∞—Ä–∫–µ—Ä—ã –º–µ–¥–∏–∞\n",
        "    if line2.startswith(\"[üì∑\") or line2.startswith(\"[üé¨\"):\n",
        "        return True\n",
        "\n",
        "    # —Å–ª—É–∂–µ–±–Ω—ã–µ\n",
        "    if line2 == \"Telegram\":\n",
        "        return True\n",
        "    if \"VIEW MESSAGE\" in line2 or \"VIEW CHANNEL\" in line2 or \"INSTANT VIEW\" in line2:\n",
        "        return True\n",
        "\n",
        "    # –º—É—Å–æ—Ä\n",
        "    if _is_time_line(line2) or _is_small_digit_line(line2):\n",
        "        return True\n",
        "\n",
        "    # URL\n",
        "    if re.fullmatch(r\"https?://\\S+\", line2):\n",
        "        return True\n",
        "\n",
        "    # —Ä–µ–∫–ª–∞–º–∞\n",
        "    if \"–†–ï–ö–õ–ê–ú–ê\" in line2:\n",
        "        return True\n",
        "    if line2.startswith(\"–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è\") or \"–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è\" in line2:\n",
        "        return True\n",
        "    if \"–ü—Ä–∏—Å–ª–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏\" in line2 or \"–ù–æ–≤–æ—Å—Ç–∏ —Å—é–¥–∞\" in line2:\n",
        "        return True\n",
        "    if \"knd.gov.ru/license\" in line2:\n",
        "        return True\n",
        "\n",
        "    # –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "    if \"@\" in line2 and any(w in line2.lower() for w in [\"bot\", \"—Ä–µ–∫–ª–∞–º–∞\", \"admin\", \"pr\", \"–ø–æ–¥–±–æ—Ä\"]):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def extract_semantic_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = _strip_markdown_links(text)\n",
        "\n",
        "    lines = [ln.strip() for ln in text.splitlines()]\n",
        "    kept = []\n",
        "    skip_after_telegram = 0\n",
        "\n",
        "    for ln in lines:\n",
        "        if not ln:\n",
        "            continue\n",
        "\n",
        "        # –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ \"Telegram\" –æ–±—ã—á–Ω–æ –∏–¥—ë—Ç —Å—Ç—Ä–æ–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞–Ω–∞–ª–∞ - —É–±–∏—Ä–∞–µ–º\n",
        "        if skip_after_telegram > 0:\n",
        "            skip_after_telegram -= 1\n",
        "            continue\n",
        "\n",
        "        ln2 = _PUA_RE.sub(\"\", ln).strip()\n",
        "\n",
        "        if ln2 == \"Telegram\":\n",
        "            skip_after_telegram = 1\n",
        "            continue\n",
        "\n",
        "        if _should_drop_line(ln2):\n",
        "            continue\n",
        "\n",
        "        # URLs –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏\n",
        "        ln2 = re.sub(r\"http\\S+|www\\.\\S+\", \" \", ln2).strip()\n",
        "\n",
        "        # –æ—á–∏—Å—Ç–∫–∞ markdown\n",
        "        ln2 = ln2.replace(\"**\", \"\").replace(\"__\", \"\")\n",
        "        ln2 = _normalize_ws(ln2)\n",
        "\n",
        "        if ln2:\n",
        "            kept.append(ln2)\n",
        "\n",
        "    return \"\\n\".join(kept).strip()\n",
        "\n",
        "def make_title_from_semantic(semantic_text: str, max_len: int = 180) -> str:\n",
        "    lines = [_normalize_ws(x) for x in semantic_text.splitlines() if _normalize_ws(x)]\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    return lines[0][:max_len]\n",
        "\n",
        "def is_digest_text(semantic_text: str) -> bool:\n",
        "    return bool(re.search(r\"(–≥–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏|–∫ —ç—Ç–æ–º—É —á–∞—Å—É|—Å–≤–æ–¥–∫–∞|–¥–∞–π–¥–∂–µ—Å—Ç|–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å)\", semantic_text, flags=re.IGNORECASE))\n",
        "\n",
        "\n",
        "# 2) –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É\n",
        "\n",
        "def _infer_base_date_from_texts(df: pd.DataFrame) -> datetime:\n",
        "    ts = []\n",
        "    for t in df[\"text\"].fillna(\"\").tolist():\n",
        "        m = _SCRAPE_TS_RE.search(t)\n",
        "        if m:\n",
        "            dt = pd.to_datetime(f\"{m.group(1)} {m.group(2)}\", errors=\"coerce\")\n",
        "            if pd.notna(dt):\n",
        "                ts.append(dt.to_pydatetime())\n",
        "    return max(ts) if ts else datetime.now()\n",
        "\n",
        "def parse_telegram_date(raw: str, base_date: datetime) -> pd.Timestamp:\n",
        "    \"\"\"\n",
        "    –º–µ—Å—è—Ü/–≥–æ–¥ –±–µ—Ä—ë–º –∏–∑ base_date. –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∞—Å—å –¥–∞—Ç–∞ ‚Äú–≤ –±—É–¥—É—â–µ–º‚Äù, —Å–¥–≤–∏–≥–∞–µ–º –Ω–∞ –º–µ—Å—è—Ü –Ω–∞–∑–∞–¥.\n",
        "    \"\"\"\n",
        "    if not isinstance(raw, str) or not raw.strip():\n",
        "        return pd.NaT\n",
        "\n",
        "    try:\n",
        "        cleaned = _PUA_RE.sub(\" \", raw)\n",
        "        cleaned = re.sub(r\"[^\\d:\\s]\", \" \", cleaned)\n",
        "        cleaned = _normalize_ws(cleaned)\n",
        "\n",
        "        day = None\n",
        "        hm = None\n",
        "        for part in cleaned.split():\n",
        "            if part.isdigit() and len(part) <= 2:\n",
        "                day = int(part)\n",
        "            elif re.fullmatch(r\"\\d{1,2}:\\d{2}\", part):\n",
        "                hm = part\n",
        "\n",
        "        if day is None or hm is None:\n",
        "            return pd.NaT\n",
        "\n",
        "        y, m = base_date.year, base_date.month\n",
        "        candidate = pd.to_datetime(f\"{y:04d}-{m:02d}-{day:02d} {hm}\", format=\"%Y-%m-%d %H:%M\", errors=\"coerce\")\n",
        "        if pd.isna(candidate):\n",
        "            return pd.NaT\n",
        "\n",
        "        if candidate.to_pydatetime() > base_date:\n",
        "            prev = (pd.Timestamp(y, m, 1) - pd.Timedelta(days=1))\n",
        "            candidate = pd.to_datetime(f\"{prev.year:04d}-{prev.month:02d}-{day:02d} {hm}\", errors=\"coerce\")\n",
        "\n",
        "        return candidate\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "\n",
        "\n",
        "# 3) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ dataframe\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame, min_semantic_chars: int = 40) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # –¥–∞—Ç—ã\n",
        "    base_date = _infer_base_date_from_texts(df)\n",
        "    df[\"published_at\"] = df[\"published_at\"].apply(lambda x: parse_telegram_date(x, base_date))\n",
        "\n",
        "    # —Ç–µ–∫—Å—Ç\n",
        "    df[\"semantic_text\"] = df[\"text\"].apply(extract_semantic_text)\n",
        "    df[\"title\"] = df[\"semantic_text\"].apply(make_title_from_semantic)\n",
        "\n",
        "    # —Ñ–ª–∞–≥–∏\n",
        "    df[\"is_digest\"] = df[\"semantic_text\"].apply(is_digest_text)\n",
        "    df[\"is_short\"] = df[\"semantic_text\"].fillna(\"\").str.len() < min_semantic_chars\n",
        "\n",
        "    # —É–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ —Å–º—ã—Å–ª–æ–≤–æ–≥–æ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è —Å –¥–æ–ø –∞–ª–µ—Ä—Ç–æ–º\n",
        "    before = len(df)\n",
        "    df = df[df[\"semantic_text\"].fillna(\"\").str.len() > 0].copy()\n",
        "    after = len(df)\n",
        "    if after < before:\n",
        "        print(f\"–í–Ω–∏–º–∞–Ω–∏–µ: —É–¥–∞–ª–µ–Ω–æ {before-after} –∑–∞–ø–∏—Å–µ–π –±–µ–∑ —Å–º—ã—Å–ª–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª—É–∂–µ–±–Ω—ã–µ/–ø—É—Å—Ç—ã–µ).\")\n",
        "\n",
        "    df.dropna(subset=[\"published_at\"], inplace=True)\n",
        "\n",
        "    df[\"full_text\"] = df[\"semantic_text\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 4) –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "\n",
        "def embed_texts(texts: list[str], model: SentenceTransformer, batch_size: int = 32) -> np.ndarray:\n",
        "    emb = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# 5) –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (1-–π –ø—Ä–æ—Ö–æ–¥)\n",
        "\n",
        "def _fit_agglom_cosine(embeddings: np.ndarray, distance_threshold: float, linkage: str = \"complete\") -> np.ndarray:\n",
        "    try:\n",
        "        model = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=distance_threshold,\n",
        "            linkage=linkage,\n",
        "            metric=\"cosine\",\n",
        "        )\n",
        "    except TypeError:\n",
        "        model = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=distance_threshold,\n",
        "            linkage=linkage,\n",
        "            affinity=\"cosine\",\n",
        "        )\n",
        "    return model.fit_predict(embeddings)\n",
        "\n",
        "def cluster_first_pass(\n",
        "    df: pd.DataFrame,\n",
        "    embeddings_all: np.ndarray,\n",
        "    dist_thr_main: float = 0.16,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    1-–π –ø—Ä–æ—Ö–æ–¥: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –¢–û–õ–¨–ö–û —Å–æ–±—ã—Ç–∏–π–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏.\n",
        "    Digest-–Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞–∑–Ω–∞—á–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ cluster_id –ø–æ–∑–∂–µ.\n",
        "    –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å—á–∏—Ç–∞–µ–º 1 —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö, —Å—é–¥–∞ –ø–µ—Ä–µ–¥–∞–µ–º embeddings_all.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"cluster_id\"] = -1\n",
        "\n",
        "    # ¬´–∏—Å—Ç–∏–Ω–Ω—ã–µ¬ª –Ω–æ–≤–æ—Å—Ç–∏\n",
        "    mask_main = (~df[\"is_short\"]) & (~df[\"is_digest\"])\n",
        "    if mask_main.any():\n",
        "        emb_main = embeddings_all[mask_main.to_numpy()]\n",
        "        labels_main = _fit_agglom_cosine(\n",
        "            emb_main,\n",
        "            distance_threshold=dist_thr_main,\n",
        "            linkage=\"complete\"\n",
        "        )\n",
        "        df.loc[mask_main, \"cluster_id\"] = labels_main\n",
        "\n",
        "    # –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ (digest –∏ short) –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º -1\n",
        "    df[\"cluster_id\"] = df[\"cluster_id\"].astype(int)\n",
        "    return df\n",
        "\n",
        "def assign_non_event_clusters(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Digest –∏ short –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ —Å—á–∏—Ç–∞–µ–º \"—Å–æ–±—ã—Ç–∏—è–º–∏\".\n",
        "    short: –∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä\n",
        "    digest: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ —á–∞—Å–∞)\n",
        "    —á—Ç–æ–±—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ \"–ì–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å\" –Ω–µ –¥—Ä–æ–±–∏–ª–∏—Å—å.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # —Å—Ç–∞—Ä—Ç–æ–≤—ã–π offset –¥–ª—è –Ω–æ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "    offset = int(df[\"cluster_id\"].max()) + 1 if (df[\"cluster_id\"].max() >= 0) else 0\n",
        "\n",
        "    # short: —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ\n",
        "    mask_s = df[\"is_short\"]\n",
        "    if mask_s.any():\n",
        "        n = int(mask_s.sum())\n",
        "        df.loc[mask_s, \"cluster_id\"] = np.arange(offset, offset + n)\n",
        "        offset += n\n",
        "\n",
        "    # digest: –ø—Ä–∞–≤–∏–ª–æ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É + –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n",
        "    mask_d = df[\"is_digest\"] & (~df[\"is_short\"])\n",
        "    if mask_d.any():\n",
        "        def norm_title(t: str) -> str:\n",
        "            t = (t or \"\").lower()\n",
        "            t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "            t = t.replace(\"–≥–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∫ —ç—Ç–æ–º—É —á–∞—Å—É\", \"–≥–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∫ —ç—Ç–æ–º—É —á–∞—Å—É\")\n",
        "            t = t.replace(\"–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å\", \"–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å\")\n",
        "            return t[:80]\n",
        "\n",
        "        tmp = df.loc[mask_d, [\"title\", \"published_at\"]].copy()\n",
        "        tmp[\"digest_key\"] = tmp[\"title\"].astype(str).apply(norm_title)\n",
        "        tmp[\"hour_bucket\"] = pd.to_datetime(tmp[\"published_at\"]).dt.floor(\"H\")\n",
        "\n",
        "\n",
        "        keys = tmp[\"digest_key\"].astype(str) + \"||\" + tmp[\"hour_bucket\"].astype(str)\n",
        "        codes, _ = pd.factorize(keys, sort=True)\n",
        "\n",
        "        df.loc[mask_d, \"cluster_id\"] = codes + offset\n",
        "        offset = int(df[\"cluster_id\"].max()) + 1\n",
        "\n",
        "    df[\"cluster_id\"] = df[\"cluster_id\"].astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "# 6) –í—ã–±–æ—Ä –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏ summary\n",
        "\n",
        "def select_canonical_and_summary(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df = df.copy()\n",
        "    df = df.sort_values([\"cluster_id\", \"published_at\"], ascending=[True, True])\n",
        "\n",
        "    df[\"is_canonical\"] = False\n",
        "    canon_idx = df.groupby(\"cluster_id\", as_index=False).head(1).index\n",
        "    df.loc[canon_idx, \"is_canonical\"] = True\n",
        "\n",
        "    sizes = df.groupby(\"cluster_id\")[\"id\"].size()\n",
        "    news_ids = df.groupby(\"cluster_id\")[\"id\"].apply(list)\n",
        "\n",
        "    canon = df[df[\"is_canonical\"]].set_index(\"cluster_id\")\n",
        "    summary = pd.DataFrame({\n",
        "        \"cluster_id\": sizes.index,\n",
        "        \"size\": sizes.values,\n",
        "        \"canonical_news_id\": canon[\"id\"],\n",
        "        \"canonical_title\": canon[\"title\"].fillna(\"\").astype(str).str.slice(0, 220),\n",
        "        \"news_ids\": news_ids.values\n",
        "    }).reset_index(drop=True)\n",
        "\n",
        "    return df, summary\n",
        "\n",
        "\n",
        "\n",
        "# 7) –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ –º—ë—Ä–¥–∂–∞ (–Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)\n",
        "\n",
        "def merge_clusters_second_pass_centroid(\n",
        "    df: pd.DataFrame,\n",
        "    embeddings_all: np.ndarray,\n",
        "    sim_threshold: float = 0.91,\n",
        "    top_k: int = 10,\n",
        "    time_window_hours: int = 96,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    2-–π –ø—Ä–æ—Ö–æ–¥: —Å–ª–∏–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞.\n",
        "    –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º ‚Äì –±–µ—Ä—ë–º embeddings_all.\n",
        "    –°–ª–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–±—ã—Ç–∏–π–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    event_mask = (~df[\"is_digest\"]) & (~df[\"is_short\"])\n",
        "    if event_mask.sum() <= 1:\n",
        "        return df\n",
        "\n",
        "    df_event = df[event_mask].copy()\n",
        "    emb_event = embeddings_all[event_mask.to_numpy()]\n",
        "\n",
        "    cluster_ids = df_event[\"cluster_id\"].to_numpy()\n",
        "    uniq = np.unique(cluster_ids)\n",
        "\n",
        "    if len(uniq) <= 1:\n",
        "        return df\n",
        "\n",
        "    centroids = []\n",
        "    times = []\n",
        "    uniq_list = []\n",
        "\n",
        "    for cid in uniq:\n",
        "        idx = np.where(cluster_ids == cid)[0]\n",
        "        c = emb_event[idx].mean(axis=0)\n",
        "        c = c / (np.linalg.norm(c) + 1e-12)\n",
        "        centroids.append(c.astype(np.float32))\n",
        "        uniq_list.append(int(cid))\n",
        "\n",
        "        t = df_event.loc[df_event[\"cluster_id\"] == cid, \"published_at\"].min()\n",
        "        times.append(pd.to_datetime(t))\n",
        "\n",
        "    centroids = np.vstack(centroids).astype(np.float32)\n",
        "    times = np.array(times, dtype=\"datetime64[ns]\")\n",
        "\n",
        "    K, D = centroids.shape\n",
        "    index = hnswlib.Index(space=\"cosine\", dim=D)\n",
        "    index.init_index(max_elements=K, ef_construction=200, M=32)\n",
        "    index.add_items(centroids, np.arange(K))\n",
        "    index.set_ef(80)\n",
        "\n",
        "    nbrs, dists = index.knn_query(centroids, k=min(top_k, K))\n",
        "    sims = 1.0 - dists\n",
        "    window = np.timedelta64(time_window_hours, \"h\")\n",
        "\n",
        "    # —Å–ª–∏—è–Ω–∏–µ —á–µ—Ä–µ–∑ DSU\n",
        "    class DSU:\n",
        "        def __init__(self, n):\n",
        "            self.p = list(range(n))\n",
        "            self.r = [0]*n\n",
        "        def find(self, x):\n",
        "            while self.p[x] != x:\n",
        "                self.p[x] = self.p[self.p[x]]\n",
        "                x = self.p[x]\n",
        "            return x\n",
        "        def union(self, a, b):\n",
        "            ra, rb = self.find(a), self.find(b)\n",
        "            if ra == rb:\n",
        "                return\n",
        "            if self.r[ra] < self.r[rb]:\n",
        "                self.p[ra] = rb\n",
        "            elif self.r[ra] > self.r[rb]:\n",
        "                self.p[rb] = ra\n",
        "            else:\n",
        "                self.p[rb] = ra\n",
        "                self.r[ra] += 1\n",
        "\n",
        "    dsu = DSU(K)\n",
        "\n",
        "    for i in range(K):\n",
        "        for j, sim in zip(nbrs[i], sims[i]):\n",
        "            j = int(j)\n",
        "            if j == i:\n",
        "                continue\n",
        "            if sim < sim_threshold:\n",
        "                continue\n",
        "            if np.abs(times[i] - times[j]) > window:\n",
        "                continue\n",
        "            dsu.union(i, j)\n",
        "\n",
        "    roots = [dsu.find(i) for i in range(K)]\n",
        "    root2new = {}\n",
        "    old2new = {}\n",
        "    new_id = 0\n",
        "    for i, r in enumerate(roots):\n",
        "        if r not in root2new:\n",
        "            root2new[r] = new_id\n",
        "            new_id += 1\n",
        "        old2new[uniq_list[i]] = root2new[r]\n",
        "\n",
        "    df.loc[event_mask, \"cluster_id\"] = df.loc[event_mask, \"cluster_id\"].map(old2new).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def reindex_cluster_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    uniq = np.sort(df[\"cluster_id\"].unique())\n",
        "    mapping = {old: new for new, old in enumerate(uniq)}\n",
        "    df[\"cluster_id\"] = df[\"cluster_id\"].map(mapping).astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 8) –ü–∞–π–ø–ª–∞–π–Ω\n",
        "\n",
        "def run_pipeline(\n",
        "    input_path: str,\n",
        "    model_name: str = \"ai-forever/sbert_large_nlu_ru\",\n",
        "    batch_size: int = 32,\n",
        "    min_semantic_chars: int = 40,\n",
        "    dist_thr_main: float = 0.16,\n",
        "    merge_sim_threshold: float = 0.92,\n",
        "    merge_time_window_hours: int = 96,\n",
        "    merge_top_k: int = 25,\n",
        "):\n",
        "    # load\n",
        "    df_raw = pd.read_json(input_path)\n",
        "    required = {\"id\", \"text\", \"source\", \"published_at\"}\n",
        "    missing = required - set(df_raw.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "\n",
        "    df = preprocess_data(df_raw, min_semantic_chars=min_semantic_chars)\n",
        "    print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df)}\")\n",
        "\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    embeddings_all = embed_texts(df[\"full_text\"].tolist(), model, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    df = cluster_first_pass(\n",
        "        df,\n",
        "        embeddings_all=embeddings_all,\n",
        "        dist_thr_main=dist_thr_main,\n",
        "    )\n",
        "\n",
        "    df = assign_non_event_clusters(df)\n",
        "\n",
        "    df = merge_clusters_second_pass_centroid(\n",
        "        df,\n",
        "        embeddings_all=embeddings_all,\n",
        "        sim_threshold=merge_sim_threshold,\n",
        "        top_k=merge_top_k,\n",
        "        time_window_hours=merge_time_window_hours,\n",
        "    )\n",
        "\n",
        "    df = reindex_cluster_ids(df)\n",
        "\n",
        "    final_df, clusters_summary_df = select_canonical_and_summary(df)\n",
        "    return final_df, clusters_summary_df\n",
        "\n",
        "\n",
        "\n",
        "# 9) –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –≤—ã–≤–æ–¥–æ–º –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º\n",
        "\n",
        "INPUT_PATH = \"tg_messages (116).json\" # tg_messages (116).json - 116 –∑–∞–ø–∏—Å–µ–π // news.json - 2000 –∑–∞–ø–∏—Å–µ–π\n",
        "\n",
        "final_df, clusters_summary_df = run_pipeline(\n",
        "    input_path=INPUT_PATH,\n",
        "    model_name=\"ai-forever/sbert_large_nlu_ru\",\n",
        "    batch_size=32,\n",
        "    min_semantic_chars=40,\n",
        "    dist_thr_main=0.16,\n",
        "    merge_sim_threshold=0.92,\n",
        "    merge_time_window_hours=96,\n",
        "    merge_top_k=25,\n",
        ")\n",
        "\n",
        "print(\"\\n–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∏—Ç–æ–≥):\", final_df[\"cluster_id\"].nunique())\n",
        "\n",
        "print(\"\\n--- –°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º ---\")\n",
        "print(\n",
        "    clusters_summary_df[[\"cluster_id\", \"size\", \"canonical_title\"]]\n",
        "      .sort_values(\"size\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\n--- –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç) ---\")\n",
        "result_columns = [\"id\", \"title\", \"source\", \"published_at\", \"cluster_id\", \"is_canonical\"]\n",
        "print(final_df[result_columns].sort_values(\"cluster_id\").head(60))\n",
        "\n",
        "final_df.to_csv(\"news_dedup_full.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "clusters_summary_df.to_csv(\"clusters_summary.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "clusters_list = (\n",
        "    clusters_summary_df[[\"cluster_id\", \"news_ids\"]]\n",
        "      .rename(columns={\"cluster_id\": \"group_id\"})\n",
        "      .to_dict(\"records\")\n",
        ")\n",
        "with open(\"clusters_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(clusters_list, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSaved files: news_dedup_full.csv, news_dedup_full.jsonl, clusters_summary.csv, clusters_summary.jsonl, clusters_list.json\")\n"
      ],
      "metadata": {
        "id": "nSlijGEt1c_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "assert (final_df.groupby(\"cluster_id\")[\"is_canonical\"].sum() == 1).all()\n",
        "\n",
        "# summary —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω —Å final_df\n",
        "canon_map = final_df.loc[final_df[\"is_canonical\"], [\"cluster_id\", \"id\"]].set_index(\"cluster_id\")[\"id\"].to_dict()\n",
        "sum_map = clusters_summary_df.set_index(\"cluster_id\")[\"canonical_news_id\"].to_dict()\n",
        "bad = [(cid, canon_map.get(cid), sum_map.get(cid)) for cid in sum_map.keys() if canon_map.get(cid) != sum_map.get(cid)]\n",
        "print(\"Canonical mismatches:\", bad[:10])\n"
      ],
      "metadata": {
        "id": "snAOxM819DzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "try:\n",
        "    emb_vis = embeddings_all\n",
        "except NameError:\n",
        "    try:\n",
        "        emb_vis = embeddings\n",
        "    except NameError:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer(\"ai-forever/sbert_large_nlu_ru\")\n",
        "        texts = final_df[\"semantic_text\"].tolist() if \"semantic_text\" in final_df.columns else final_df[\"text\"].astype(str).tolist()\n",
        "        emb_vis = model.encode(texts, batch_size=32, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.asarray(emb_vis)\n",
        "\n",
        "try:\n",
        "    import umap\n",
        "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
        "    X2 = reducer.fit_transform(X)\n",
        "    method = \"UMAP\"\n",
        "except Exception:\n",
        "    from sklearn.decomposition import PCA\n",
        "    X2 = PCA(n_components=2, random_state=42).fit_transform(X)\n",
        "    method = \"PCA(2D)\"\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X2[:, 0], X2[:, 1], s=12, c=final_df[\"cluster_id\"].astype(int), alpha=0.8)\n",
        "plt.title(f\"Clusters visualization ({method}) | N={len(final_df)} | K={final_df['cluster_id'].nunique()}\")\n",
        "plt.xlabel(\"dim-1\")\n",
        "plt.ylabel(\"dim-2\")\n",
        "plt.colorbar(label=\"cluster_id\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "canon_mask = final_df[\"is_canonical\"].astype(bool).values\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X2[:, 0], X2[:, 1], s=10, c=final_df[\"cluster_id\"].astype(int), alpha=0.25)\n",
        "plt.scatter(X2[canon_mask, 0], X2[canon_mask, 1], s=120, edgecolors=\"black\", linewidths=1.0,\n",
        "            c=final_df.loc[canon_mask, \"cluster_id\"].astype(int))\n",
        "plt.title(f\"Canonical points highlighted ({method})\")\n",
        "plt.xlabel(\"dim-1\")\n",
        "plt.ylabel(\"dim-2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WJjDwPwS7qC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
